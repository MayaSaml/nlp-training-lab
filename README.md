# NLP Training Lab (PyTorch)

Welcome! This repository is part of an educational series where I explore, document, and teach core NLP concepts using PyTorch â€” from foundational ideas like RNNs to more advanced topics like Transformers and LLM fine-tuning.

Each notebook is written with clarity and structure, meant to balance both practical understanding and professional code style. While some examples are minimal or toy-sized, the focus is on understanding why and how things work â€” a crucial step for mastering real-world NLP.

---

## ðŸ§  Folders Index

| Notebook                         | Description                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| `01_pytorch_crashcourse`  | A beginner-friendly crash course in PyTorch, covering tensors, gradients, and simple models (including a custom `SimpleNet`) |
| `02_rnn`      | Toy RNN-based models for 3 NLP tasks (language modeling, tagging, classification) |
| `03_transformers`      | Examples to showcase multihead self-attention tensor calculations with positional encoding  |
| *More coming soon*              | Transformers, Attention, LLM Fine-Tuning, RAG, and more |

---

## ðŸ”§ How to Use

You can run the notebooks in:
- Jupyter Notebook or JupyterLab
- Google Colab

No special data or GPU is required â€” all examples are self-contained and CPU-friendly.

---

## ðŸ’¡ About This Project

> ðŸ’¬ *â€œReports of the death of programming are greatly exaggerated.â€*  
> â€“ Bill Gates (2024), on the future of software in the age of AI

Here, I break down complex NLP and LLM topics, implement core architectures, and share insights from my 4+ years of experience in data science, banking AI, and deep learning research.

I believe that:
- In an age where everyone is *using* AI, few truly *understand* it.
- Learning the internals of Transformers, attention, embeddings, and LLM pipelines is what sets great engineers apart.
- Clarity is power â€” and sharing that clarity makes the whole field stronger.

This repo is:
- ðŸ§ª A space where I explore and document foundational and advanced concepts.
- ðŸ“š A resource for those seeking hands-on, annotated PyTorch examples.
- ðŸ§­ A reflection of my belief in **deep work**, **mathematical grounding**, and **real-world application**.

> ðŸ§© AI is not magic. It's math, logic, and engineering â€” and we *can* understand it.  
> If we want to shape the future, we need to know how the tools work â€” from the inside out.

Whether you're a fellow learner or someone getting lost in the sea of LLM buzzwords â€” I hope this repo helps you rediscover the beauty of clear thinking and clean code.



