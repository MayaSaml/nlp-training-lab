# NLP Training Lab (PyTorch)

Welcome! This repository is part of an educational series where I explore, document, and teach core NLP concepts using PyTorch â€” from foundational ideas like RNNs to more advanced topics like Transformers and LLM fine-tuning.

Each notebook is written with clarity and structure, meant to balance both practical understanding and professional code style. While some examples are minimal or toy-sized, the focus is on understanding why and how things work â€” a crucial step for mastering real-world NLP.

---

## ðŸ§  Notebook Index

| Notebook                         | Description                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| `01_pytorch_crashcourse.ipynb`  | A beginner-friendly crash course in PyTorch, covering tensors, gradients, and simple models (including a custom `SimpleNet`) |
| `rnn_toy_examples.ipynb`        | Toy RNN-based models for 3 NLP tasks (language modeling, tagging, classification) |
| *More coming soon*              | Transformers, Attention, LLM Fine-Tuning, RAG, and more |

---

## ðŸ”§ How to Use

You can run the notebooks in:
- Jupyter Notebook or JupyterLab
- Google Colab

No special data or GPU is required â€” all examples are self-contained and CPU-friendly.

---

## ðŸ’¡ About This Project

This repo reflects my teaching journey in NLP. Itâ€™s here to:
- Help others build foundational intuition
- Practice clear and effective PyTorch implementations
- Serve as a stepping stone to more complex topics

> Whether you're a beginner or brushing up before diving into LLMs, I hope you find something useful here.
