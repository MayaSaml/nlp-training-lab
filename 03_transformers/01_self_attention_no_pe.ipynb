{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1034f35-b571-48ed-9b9a-ae57ba03d0ec",
   "metadata": {},
   "source": [
    "### Self-Attention \n",
    "\n",
    "**Sentence:** `\"I understand this\"`  \n",
    "**Setup:** 8 self-attention heads (like in the original Transformer paper)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Token-level attention computation\n",
    "- Multi-head projection (Q, K, V)\n",
    "- Scaled dot-product attention per head\n",
    "- Head concatenation and final projection\n",
    "\n",
    "Result: A `(3, 512)` matrix — the output that would flow into LayerNorm in a full Transformer encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "436c5581-301e-4c75-bfbf-58ec46fce7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_503</th>\n",
       "      <th>dim_504</th>\n",
       "      <th>dim_505</th>\n",
       "      <th>dim_506</th>\n",
       "      <th>dim_507</th>\n",
       "      <th>dim_508</th>\n",
       "      <th>dim_509</th>\n",
       "      <th>dim_510</th>\n",
       "      <th>dim_511</th>\n",
       "      <th>dim_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>262.300995</td>\n",
       "      <td>330.110840</td>\n",
       "      <td>638.167297</td>\n",
       "      <td>340.702942</td>\n",
       "      <td>-56.639923</td>\n",
       "      <td>64.868973</td>\n",
       "      <td>-184.489624</td>\n",
       "      <td>53.167908</td>\n",
       "      <td>475.184814</td>\n",
       "      <td>565.570740</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.628601</td>\n",
       "      <td>-35.616707</td>\n",
       "      <td>89.243835</td>\n",
       "      <td>-92.533752</td>\n",
       "      <td>201.338257</td>\n",
       "      <td>263.308960</td>\n",
       "      <td>-140.372284</td>\n",
       "      <td>-152.854126</td>\n",
       "      <td>417.635254</td>\n",
       "      <td>-112.275139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>431.989807</td>\n",
       "      <td>487.913483</td>\n",
       "      <td>200.713760</td>\n",
       "      <td>360.062500</td>\n",
       "      <td>265.537292</td>\n",
       "      <td>29.054657</td>\n",
       "      <td>-531.808289</td>\n",
       "      <td>790.458862</td>\n",
       "      <td>-510.066101</td>\n",
       "      <td>567.686401</td>\n",
       "      <td>...</td>\n",
       "      <td>327.851227</td>\n",
       "      <td>850.667603</td>\n",
       "      <td>-91.303284</td>\n",
       "      <td>188.982437</td>\n",
       "      <td>85.921738</td>\n",
       "      <td>-161.513458</td>\n",
       "      <td>-92.091789</td>\n",
       "      <td>-575.929688</td>\n",
       "      <td>201.806549</td>\n",
       "      <td>158.486633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-306.064484</td>\n",
       "      <td>171.983154</td>\n",
       "      <td>-185.567963</td>\n",
       "      <td>320.279663</td>\n",
       "      <td>-387.239624</td>\n",
       "      <td>-92.568375</td>\n",
       "      <td>-1064.611450</td>\n",
       "      <td>577.439697</td>\n",
       "      <td>-114.290245</td>\n",
       "      <td>129.013046</td>\n",
       "      <td>...</td>\n",
       "      <td>378.655914</td>\n",
       "      <td>485.287598</td>\n",
       "      <td>-649.637085</td>\n",
       "      <td>-312.299927</td>\n",
       "      <td>-138.976074</td>\n",
       "      <td>-169.407806</td>\n",
       "      <td>175.710449</td>\n",
       "      <td>52.884216</td>\n",
       "      <td>871.741089</td>\n",
       "      <td>-412.337372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dim_1       dim_2       dim_3       dim_4       dim_5  \\\n",
       "I           262.300995  330.110840  638.167297  340.702942  -56.639923   \n",
       "understand  431.989807  487.913483  200.713760  360.062500  265.537292   \n",
       "this       -306.064484  171.983154 -185.567963  320.279663 -387.239624   \n",
       "\n",
       "                dim_6        dim_7       dim_8       dim_9      dim_10  ...  \\\n",
       "I           64.868973  -184.489624   53.167908  475.184814  565.570740  ...   \n",
       "understand  29.054657  -531.808289  790.458862 -510.066101  567.686401  ...   \n",
       "this       -92.568375 -1064.611450  577.439697 -114.290245  129.013046  ...   \n",
       "\n",
       "               dim_503     dim_504     dim_505     dim_506     dim_507  \\\n",
       "I           -13.628601  -35.616707   89.243835  -92.533752  201.338257   \n",
       "understand  327.851227  850.667603  -91.303284  188.982437   85.921738   \n",
       "this        378.655914  485.287598 -649.637085 -312.299927 -138.976074   \n",
       "\n",
       "               dim_508     dim_509     dim_510     dim_511     dim_512  \n",
       "I           263.308960 -140.372284 -152.854126  417.635254 -112.275139  \n",
       "understand -161.513458  -92.091789 -575.929688  201.806549  158.486633  \n",
       "this       -169.407806  175.710449   52.884216  871.741089 -412.337372  \n",
       "\n",
       "[3 rows x 512 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Step 1: Define input\n",
    "# ------------------------\n",
    "\n",
    "tokens = [\"I\", \"understand\", \"this\"]     # Sequence of 3 tokens\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# Model dimensions (matching the Transformer architecture)\n",
    "d_model = 512                            # Total embedding size (512 dims)\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads              # Size of each head (512 / 8 = 64)\n",
    "\n",
    "# Simulate word embeddings (random for demo)\n",
    "X = torch.randn(seq_len, d_model)       # Shape: (3, 512)\n",
    "\n",
    "# ------------------------\n",
    "# Step 2: Multi-Head Attention\n",
    "# ------------------------\n",
    "\n",
    "Z_heads = []                             # Store outputs from each head\n",
    "W_Q_list, W_K_list, W_V_list = [], [], []  # Store projection weights per head (optional for inspection)\n",
    "\n",
    "for head in range(num_heads):\n",
    "    # Simulated learned projection matrices (different per head here for demo)\n",
    "    W_Q = torch.randn(d_model, d_k)      # Shape: (512, 64)\n",
    "    W_K = torch.randn(d_model, d_k)      # Shape: (512, 64)\n",
    "    W_V = torch.randn(d_model, d_k)      # Shape: (512, 64)\n",
    "\n",
    "    # Project input embeddings to Q, K, V\n",
    "    Q = X @ W_Q                          # Shape: (3, 64)\n",
    "    K = X @ W_K                          # Shape: (3, 64)\n",
    "    V = X @ W_V                          # Shape: (3, 64)\n",
    "\n",
    "    # Store weight matrices if needed later\n",
    "    W_Q_list.append(W_Q)\n",
    "    W_K_list.append(W_K)\n",
    "    W_V_list.append(W_V)\n",
    "\n",
    "    # Compute scaled dot-product attention\n",
    "    # Formula: Attention(Q, K, V) = softmax(Q × Kᵀ / √d_k) × V\n",
    "    scores = Q @ K.T / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))  # Shape: (3, 3)\n",
    "    attn_weights = F.softmax(scores, dim=-1)                                # Shape: (3, 3)\n",
    "    Z = attn_weights @ V                                                   # Shape: (3, 64)\n",
    "\n",
    "    Z_heads.append(Z)\n",
    "\n",
    "# ------------------------\n",
    "# Step 3: Concatenate Head Outputs\n",
    "# ------------------------\n",
    "\n",
    "# Stack all head outputs side by side\n",
    "# Resulting shape: (3 tokens, 64 × 8 heads) → (3, 512)\n",
    "Z_concat = torch.cat(Z_heads, dim=-1)\n",
    "\n",
    "# ------------------------\n",
    "# Step 4: Final Linear Projection\n",
    "# ------------------------\n",
    "\n",
    "# Final projection matrix to mix information across heads\n",
    "W_O = torch.randn(d_model, d_model)     # Shape: (512, 512)\n",
    "output = Z_concat @ W_O                 # Shape: (3, 512)\n",
    "\n",
    "# ------------------------\n",
    "# Step 5: Visualize Output\n",
    "# ------------------------\n",
    "\n",
    "# Wrap the output in a DataFrame for readability\n",
    "df = pd.DataFrame(\n",
    "    output.detach().numpy(),\n",
    "    index=tokens,\n",
    "    columns=[f\"dim_{i+1}\" for i in range(d_model)]\n",
    ")\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
