{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac4953e-cc1c-4ae4-96d2-b645b19928ea",
   "metadata": {},
   "source": [
    "# Full Transformer Encoder Stack\n",
    "\n",
    "Implements a full stack of **8 Transformer encoder blocks** using PyTorch.\n",
    "\n",
    "Each block consists of:\n",
    "- Multi-head self-attention (8 heads, d_k = 64)\n",
    "- Residual connections + LayerNorm\n",
    "- Feedforward network (512 → 2048 → 512)\n",
    "\n",
    "The sentence `\"I understand this\"` is passed through the stack, starting with random embeddings and sinusoidal positional encoding.\n",
    "\n",
    "Output: Final token representations of shape `(3, 512)` — enriched by 8 rounds of context-aware attention and feedforward refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e1d23b-a746-45b1-b718-02958393e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ------------------------\n",
    "# Positional Encoding Function (Same as in previous notebooks)\n",
    "# ------------------------\n",
    "def add_positional_encoding(X):\n",
    "    seq_len, d_model = X.shape\n",
    "    position = torch.arange(seq_len).unsqueeze(1)  # (seq_len, 1)\n",
    "    i = torch.arange(d_model).unsqueeze(0)         # (1, d_model)\n",
    "    angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "    angle_rads = position * angle_rates\n",
    "\n",
    "    PE = torch.zeros_like(angle_rads)\n",
    "    PE[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "    PE[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    return X + PE\n",
    "\n",
    "# ------------------------\n",
    "# Encoder Block (Self-Attn + FFN + LayerNorm)\n",
    "# ------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, ffn_hidden=2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "        self.W_K = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "        self.W_V = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Attention\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            Q = self.W_Q[i](x)\n",
    "            K = self.W_K[i](x)\n",
    "            V = self.W_V[i](x)\n",
    "\n",
    "            scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            Z = attn @ V\n",
    "            heads.append(Z)\n",
    "\n",
    "        concat = torch.cat(heads, dim=-1)\n",
    "        attn_out = self.W_O(concat)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(x)\n",
    "        out = self.norm2(x + ffn_out)\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# Full Transformer Encoder Stack (8 layers)\n",
    "# ------------------------\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=8, d_model=512, num_heads=8, ffn_hidden=2048):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, ffn_hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------\n",
    "# Example Run\n",
    "# ------------------------\n",
    "tokens = [\"I\", \"understand\", \"this\"]\n",
    "seq_len = len(tokens)\n",
    "d_model = 512\n",
    "\n",
    "# Simulate input embeddings\n",
    "X = torch.randn(seq_len, d_model)\n",
    "X = add_positional_encoding(X)\n",
    "\n",
    "# Run through 8-layer encoder\n",
    "encoder = TransformerEncoder(num_layers=8)\n",
    "encoder_output = encoder(X)  # (3, 512)\n",
    "\n",
    "# Visualize result\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(encoder_output.detach().numpy(), index=tokens, columns=[f\"dim_{i+1}\" for i in range(d_model)])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d66b1-a994-4791-8d7b-8cc42176cd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
