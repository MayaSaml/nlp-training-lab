{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d906c7-2097-4584-834a-7e3adc2fabea",
   "metadata": {},
   "source": [
    "# Transformer Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa38e86b-9e76-40d2-9f23-1adada82c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import PositionalEncoding, TransformerEncoder, TransformerDecoder\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, ffn_hidden=2048, num_layers=8, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer for both encoder and decoder inputs\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "\n",
    "        # Encoder and decoder stacks\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, ffn_hidden)\n",
    "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, ffn_hidden)\n",
    "\n",
    "        # Final output layer: projects to vocab size\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, look_ahead_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: (src_seq_len,) or (batch, src_seq_len)\n",
    "        tgt_tokens: (tgt_seq_len,) or (batch, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        # Embed + add positional encoding\n",
    "        src_embed = self.token_embedding(src_tokens)          # (seq_len, d_model)\n",
    "        src_embed = self.positional_encoding(src_embed)\n",
    "\n",
    "        tgt_embed = self.token_embedding(tgt_tokens)          # (seq_len, d_model)\n",
    "        tgt_embed = self.positional_encoding(tgt_embed)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(src_embed)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_output = self.decoder(tgt_embed, encoder_output, look_ahead_mask)\n",
    "\n",
    "        # Project to vocab logits\n",
    "        logits = self.output_layer(decoder_output)  # (seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc85b2c-b81a-4c8d-906a-78140d2db66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Full Transformer Model with Embeddings\n",
    "# ------------------------\n",
    "class TransformerWithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, d_model=512, num_heads=8, ffn_hidden=2048, num_layers=6, max_len=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding layers for source and target\n",
    "        self.src_embedding = nn.Embedding(vocab_size_src, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size_tgt, d_model)\n",
    "\n",
    "        # Shared positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Transformer encoder-decoder\n",
    "        self.transformer = Transformer(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            ffn_hidden=ffn_hidden\n",
    "        )\n",
    "\n",
    "        # Final linear layer to project decoder output to vocabulary\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size_tgt)\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids):\n",
    "        # src_ids: (src_seq_len,)\n",
    "        # tgt_ids: (tgt_seq_len,)\n",
    "\n",
    "        # Embed and encode positional info\n",
    "        src = self.pos_encoding(self.src_embedding(src_ids) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)))\n",
    "        tgt = self.pos_encoding(self.tgt_embedding(tgt_ids) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)))\n",
    "\n",
    "        # Create look-ahead mask\n",
    "        look_ahead_mask = self.generate_look_ahead_mask(tgt.size(0))\n",
    "\n",
    "        # Transformer forward\n",
    "        output = self.transformer(src, tgt, look_ahead_mask)\n",
    "\n",
    "        # Project to vocab size\n",
    "        logits = self.output_linear(output)  # Shape: (tgt_seq_len, vocab_size_tgt)\n",
    "        return logits\n",
    "\n",
    "    def generate_look_ahead_mask(self, size):\n",
    "        # Creates a lower-triangular matrix (1s in allowed positions, 0s elsewhere)\n",
    "        return torch.tril(torch.ones(size, size)).bool()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
