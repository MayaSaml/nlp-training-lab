{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469c1184-57bd-499c-9b2b-d67ec05fa668",
   "metadata": {},
   "source": [
    "### Minimal PyTorch Simulation of Multi-Head Self-Attention with Positional Encoding\n",
    "\n",
    "**Sentence:** `\"I understand this\"`  \n",
    "**Setup:** 8 self-attention heads (like in the original Transformer paper)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Token-level attention with added **positional information**\n",
    "- Multi-head projection (Q, K, V)\n",
    "- Scaled dot-product attention per head\n",
    "- Head concatenation and final projection\n",
    "\n",
    "Result: A `(3, 512)` matrix — attention-enhanced token representations, enriched with position info.  \n",
    "This output would flow into **LayerNorm** and the feedforward block in a full Transformer encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e152ad4-1fb1-425f-b38d-5fd040bc5b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_503</th>\n",
       "      <th>dim_504</th>\n",
       "      <th>dim_505</th>\n",
       "      <th>dim_506</th>\n",
       "      <th>dim_507</th>\n",
       "      <th>dim_508</th>\n",
       "      <th>dim_509</th>\n",
       "      <th>dim_510</th>\n",
       "      <th>dim_511</th>\n",
       "      <th>dim_512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>716.715271</td>\n",
       "      <td>-470.942078</td>\n",
       "      <td>981.513916</td>\n",
       "      <td>-486.821106</td>\n",
       "      <td>295.027679</td>\n",
       "      <td>-38.370850</td>\n",
       "      <td>642.217346</td>\n",
       "      <td>337.909454</td>\n",
       "      <td>208.694931</td>\n",
       "      <td>250.257172</td>\n",
       "      <td>...</td>\n",
       "      <td>62.868011</td>\n",
       "      <td>-702.726685</td>\n",
       "      <td>223.246704</td>\n",
       "      <td>1489.421143</td>\n",
       "      <td>-743.896118</td>\n",
       "      <td>-131.536102</td>\n",
       "      <td>1059.948730</td>\n",
       "      <td>-109.258835</td>\n",
       "      <td>610.171631</td>\n",
       "      <td>185.760986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>566.205017</td>\n",
       "      <td>-451.209015</td>\n",
       "      <td>907.756226</td>\n",
       "      <td>-667.466125</td>\n",
       "      <td>619.895508</td>\n",
       "      <td>21.903992</td>\n",
       "      <td>763.903015</td>\n",
       "      <td>329.733673</td>\n",
       "      <td>-709.243896</td>\n",
       "      <td>204.303772</td>\n",
       "      <td>...</td>\n",
       "      <td>265.490234</td>\n",
       "      <td>-1070.703369</td>\n",
       "      <td>-476.322083</td>\n",
       "      <td>1152.886841</td>\n",
       "      <td>-239.545593</td>\n",
       "      <td>-326.701416</td>\n",
       "      <td>384.493835</td>\n",
       "      <td>404.269409</td>\n",
       "      <td>867.338257</td>\n",
       "      <td>-139.813889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-580.703735</td>\n",
       "      <td>-93.404175</td>\n",
       "      <td>655.662537</td>\n",
       "      <td>-361.904022</td>\n",
       "      <td>630.690430</td>\n",
       "      <td>-51.293945</td>\n",
       "      <td>1020.200134</td>\n",
       "      <td>-57.180573</td>\n",
       "      <td>-398.777771</td>\n",
       "      <td>493.941223</td>\n",
       "      <td>...</td>\n",
       "      <td>105.204384</td>\n",
       "      <td>-853.463989</td>\n",
       "      <td>-233.807693</td>\n",
       "      <td>1281.146729</td>\n",
       "      <td>-559.940125</td>\n",
       "      <td>-49.248718</td>\n",
       "      <td>-233.115997</td>\n",
       "      <td>-191.494354</td>\n",
       "      <td>883.868469</td>\n",
       "      <td>366.606354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dim_1       dim_2       dim_3       dim_4       dim_5  \\\n",
       "I           716.715271 -470.942078  981.513916 -486.821106  295.027679   \n",
       "understand  566.205017 -451.209015  907.756226 -667.466125  619.895508   \n",
       "this       -580.703735  -93.404175  655.662537 -361.904022  630.690430   \n",
       "\n",
       "                dim_6        dim_7       dim_8       dim_9      dim_10  ...  \\\n",
       "I          -38.370850   642.217346  337.909454  208.694931  250.257172  ...   \n",
       "understand  21.903992   763.903015  329.733673 -709.243896  204.303772  ...   \n",
       "this       -51.293945  1020.200134  -57.180573 -398.777771  493.941223  ...   \n",
       "\n",
       "               dim_503      dim_504     dim_505      dim_506     dim_507  \\\n",
       "I            62.868011  -702.726685  223.246704  1489.421143 -743.896118   \n",
       "understand  265.490234 -1070.703369 -476.322083  1152.886841 -239.545593   \n",
       "this        105.204384  -853.463989 -233.807693  1281.146729 -559.940125   \n",
       "\n",
       "               dim_508      dim_509     dim_510     dim_511     dim_512  \n",
       "I          -131.536102  1059.948730 -109.258835  610.171631  185.760986  \n",
       "understand -326.701416   384.493835  404.269409  867.338257 -139.813889  \n",
       "this        -49.248718  -233.115997 -191.494354  883.868469  366.606354  \n",
       "\n",
       "[3 rows x 512 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Step 1: Define input\n",
    "# ------------------------\n",
    "\n",
    "tokens = [\"I\", \"understand\", \"this\"]     # Sequence of 3 tokens\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# Model dimensions (matching the Transformer architecture)\n",
    "d_model = 512                            # Total embedding size (512 dims)\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads              # Size of each head (512 / 8 = 64)\n",
    "\n",
    "# Simulate word embeddings (random for demo)\n",
    "X = torch.randn(seq_len, d_model)       # Shape: (3, 512)\n",
    "\n",
    "# ------------------------\n",
    "# Step 2: Add Positional Encoding\n",
    "# ------------------------\n",
    "\n",
    "position = torch.arange(seq_len).unsqueeze(1)        # (3, 1)\n",
    "i = torch.arange(d_model).unsqueeze(0)               # (1, 512)\n",
    "angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "angle_rads = position * angle_rates\n",
    "\n",
    "PE = torch.zeros_like(angle_rads)\n",
    "PE[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "PE[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "X = X + PE    # Add PE to input embeddings\n",
    "\n",
    "# ------------------------\n",
    "# Step 3: Multi-Head Attention\n",
    "# ------------------------\n",
    "\n",
    "Z_heads = []                             # Store outputs from each head\n",
    "W_Q_list, W_K_list, W_V_list = [], [], []  # (Optional: store for inspection)\n",
    "\n",
    "for head in range(num_heads):\n",
    "    # Simulated learned projection matrices (random here for demo)\n",
    "    W_Q = torch.randn(d_model, d_k)      # (512, 64)\n",
    "    W_K = torch.randn(d_model, d_k)\n",
    "    W_V = torch.randn(d_model, d_k)\n",
    "\n",
    "    Q = X @ W_Q                          # (3, 64)\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "\n",
    "    scores = Q @ K.T / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))  # (3, 3)\n",
    "    attn_weights = F.softmax(scores, dim=-1)                                # (3, 3)\n",
    "    Z = attn_weights @ V                                                   # (3, 64)\n",
    "\n",
    "    Z_heads.append(Z)\n",
    "\n",
    "# ------------------------\n",
    "# Step 4: Concatenate Head Outputs\n",
    "# ------------------------\n",
    "\n",
    "Z_concat = torch.cat(Z_heads, dim=-1)   # (3, 512)\n",
    "\n",
    "# ------------------------\n",
    "# Step 5: Final Linear Projection\n",
    "# ------------------------\n",
    "\n",
    "W_O = torch.randn(d_model, d_model)     # (512, 512)\n",
    "output = Z_concat @ W_O                 # (3, 512)\n",
    "\n",
    "# ------------------------\n",
    "# Step 6: Visualize Output\n",
    "# ------------------------\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    output.detach().numpy(),\n",
    "    index=tokens,\n",
    "    columns=[f\"dim_{i+1}\" for i in range(d_model)]\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53e0bf-b87e-47f1-a273-78f5993b8349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp",
   "language": "python",
   "name": "gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
