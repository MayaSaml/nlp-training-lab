{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4870b0",
   "metadata": {},
   "source": [
    "# Text Generation with Phi-3\n",
    "This notebook introduces basic text generation using a modern instruction-tuned LLM: Microsoft Phi-3.\n",
    "It demonstrates how temperature and sampling settings impact creativity in generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c52bba",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Libraries (Only for Colab or Local Setup)\n",
    "```python\n",
    "# !pip install torch==2.3.1 transformers==4.41.2 sentence-transformers==3.0.1 \\\n",
    "#     matplotlib==3.9.0 scikit-learn==1.5.0 sentencepiece==0.2.0 \\\n",
    "#     nltk==3.8.1 evaluate==0.4.2 scipy==1.15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcac422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"your_huggingface_token_here\")  # Replace with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4ded7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.88s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading Phi-3 model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582601e",
   "metadata": {},
   "source": [
    "**About the model:**\n",
    "Phi-3 Mini is a small instruction-tuned LLM by Microsoft, optimized for fast and efficient inference.\n",
    "Despite its small size (~1.3B parameters), it performs very well thanks to highly curated training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0251954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': 'Tell me a funny joke about cats'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741e8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Deterministic Output — do_sample=False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't cats play poker in the jungle? Too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"\\n[Deterministic Output — do_sample=False]\")\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374e5af",
   "metadata": {},
   "source": [
    "### Step 2: Lower Temperature Sampling\n",
    "- Temperature controls randomness:\n",
    "  - Lower = safer, more predictable\n",
    "  - Higher = more diverse, creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b0bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Low Temperature Sampling — temperature=0.1] - probably the same joke\n",
      " Why don't cats play poker in the jungle? Too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n[Low Temperature Sampling — temperature=0.1] - probably the same joke\")\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81739739",
   "metadata": {},
   "source": [
    "### Step 3: Higher Temperature with Sampling Tricks\n",
    "- `temperature`: how adventurous to be\n",
    "- `top_k`: limit to top K tokens\n",
    "- `repetition_penalty`: discourage repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90672d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Creative Sampling — temperature=0.6, top_k=50, repetition_penalty=1.2]\n",
      " Why don't you ever play hide and seek with your cat?\n",
      "Because good luck hiding when they find it by the smell of fish! (This brings humor to feline tracking skills, often associated in jokes.)\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "print(\"\\n[Creative Sampling — temperature=0.6, top_k=50, repetition_penalty=1.2]\")\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f90740",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Deterministic = repeatable\n",
    "- Sampling = more creativity\n",
    "- Phi-3 = great mix of speed + quality\n",
    "\n",
    "Try more prompts:\n",
    "- \"Explain transformers to a 5-year-old\"\n",
    "- \"Write a haiku about machine learning\"\n",
    "- \"What is RAG in large language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db2e13-afe3-42e4-b7bf-18852adb07d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
